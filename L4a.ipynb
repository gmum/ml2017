{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wzór Bayesa\n",
    "\n",
    "A - parametry rozkładu.\n",
    "B - obserwacja.\n",
    "\n",
    "$$P(A\\mid B) = \\frac{P(B\\mid A)\\,P(A)}{P(B)}$$\n",
    "$$P(B) = {\\sum_j P(B\\mid A_j) P(A_j)}$$\n",
    "$$P(A_i\\mid B) = \\frac{P(B\\mid A_i)\\,P(A_i)}{\\sum\\limits_j P(B\\mid A_j)\\,P(A_j)}$$\n",
    "\n",
    "Prawdopodobieństwa $P(A_i)$ po prawej stronie wzoru nazywamy wiedzą a priori, $P(A_i\\mid B)$ po lewej wiedzą a posteriori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozkład prawdopodobieństwa na rozkładach prawdopodobieństwa\n",
    "\n",
    "Przypominamy na przykładzie monety:\n",
    "\n",
    "Moneta jest opisana rozkładem prawdopodobieństwa na dwóch możliwych zdarzeniach elementarnych (wynikach rzutu) {ORZEŁ, RESZKA}:\n",
    "- $P(\\mathrm{ORZEŁ}) = \\theta$,\n",
    "- $P(\\mathrm{RESZKA}) = 1 - \\theta$.\n",
    "\n",
    "$P$ jest \"wbudowaną\" własnością monety, jej cechą (nieznaną).\n",
    "\n",
    "Na tych rozkładach robimy drugi rozkład prawdopodobieństwa:\n",
    "- $\\mathcal{P}(\\theta)$.\n",
    "\n",
    "$\\mathcal{P}$ reprezentuje stan naszej wiedzy o świecie\n",
    "\n",
    "Rozkłady $P$ **nie ulegają zmianie**. Jest to rodzina wszystkich możliwych rozkładów, które bierzemy pod uwagę.\n",
    "\n",
    "Rozkład $\\mathcal{P}$ **ulega zmianie** po każdej nowej obserwacji.\n",
    "\n",
    "Przy dużej liczbie obserwacji cała gęstość rozkładu $\\mathcal{P}$ zaczyna koncentrować się bardzo blisko wokół prawdziwego $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podstawowe metody wnioskowania\n",
    "\n",
    "#### Funkcja likelihood\n",
    "\n",
    "$$L(\\theta|\\mathrm{obserwacje}) := P(\\mathrm{obserwacje}|\\theta)$$\n",
    "\n",
    "#### Maximum likelihood (ML)\n",
    "\n",
    "Dobieramy $\\tilde{\\theta}$ tak, aby zmaksymalizować $L$.\n",
    "\n",
    "Innymi słowy wybieramy taką wartość $\\tilde{\\theta}$, przy której mielibyśmy największe szanse na zobaczenie właśnie takich obserwacji.\n",
    "\n",
    "$$p(x) = p(x|\\tilde{\\theta}) = P_{\\tilde{\\theta}}(x)$$\n",
    "\n",
    "#### Maximum a posteriori (MAP)\n",
    "\n",
    "1. Mamy dany pewien rozkład a priori oraz obserwacje.\n",
    "2. Korzystając ze wzoru Bayesa obliczamy rozkład a posteriori.\n",
    "3. Wybieramy $\\tilde{\\theta}$, które maksymalizuje prawdopodobieństwo a posteriori.\n",
    "\n",
    "$$p(x) = p(x|\\tilde{\\theta}) = P_{\\tilde{\\theta}}(x)$$\n",
    "\n",
    "#### Posterior predictive distribution (PPD)\n",
    "\n",
    "Prawdopodobieństwo konkretnej obserwacji $x$ to całka po wszystkich możliwych $\\theta$ (rozkład a posteriori) z prawdopodobieństwa $x$ przy tym $\\theta$.\n",
    "\n",
    "$$p(x) = \\int_{\\theta} p(x|\\theta) \\, \\mathcal{P}(\\theta|\\mathrm{obserwacje},\\mathrm{prior}) \\operatorname{d}\\!\\theta$$\n",
    "\n",
    "#### Wzajemne związki\n",
    "\n",
    "1. PPD jest najlepszy, ale najtrudniejszy do policzenia.\n",
    "2. MAP przy dużej liczbie obserwacji aproksymuje PPD.\n",
    "3. ML z regularyzacją ma aproksymować MAP.\n",
    "4. Jeśli przyjmiemy a priori rozkład jednostajny, to ML == MAP, co w prosty sposób wynika ze wzorów (intuicyjnie cała wiedza pochodzi tylko z obserwacji, więc całą pracę wykonuje funkcja likelihood). Na ten fakt zwrócił uwagę Krzysiek Gallas, bardzo dziękujemy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odwracanie macierzy\n",
    "\n",
    "#### Odwracanie macierzy dodatnio określonej\n",
    "\n",
    "http://scicomp.stackexchange.com/questions/3188/dealing-with-the-inverse-of-a-positive-definite-symmetric-covariance-matrix\n",
    "\n",
    "Rozkład Choleskiego\n",
    "https://pl.wikipedia.org/wiki/Rozk%C5%82ad_Choleskiego\n",
    "\n",
    "numpy używa SVD zamiast Cholesky'ego\n",
    "https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/mtrand.pyx#L4496\n",
    "\n",
    "#### Moore-Penrose pseudoinverse\n",
    "\n",
    "Definicja:\n",
    "https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse#Definition\n",
    "\n",
    "Jeśli macierz $A$ ma liniowo niezależne kolumny (zazwyczaj tak jest, gdy $A$ jest macierzą danych o wymiarach (nb_samples, nb_features) i np. nb_samples >> nb_features)), to można ją zapisać wzorem\n",
    "\n",
    "$A^+ = (A^TA)^{-1}A^T$\n",
    "\n",
    "Przykładowy paper opisujący szybkie liczenie\n",
    "https://arxiv.org/pdf/1102.1845.pdf\n",
    "\n",
    "numpy używa SVD\n",
    "https://github.com/numpy/numpy/blob/master/numpy/linalg/linalg.py#L1614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
