{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation - omówienie warstw\n",
    "\n",
    "Dla każdego elementu sieci, który będziemy używać, omówimy:\n",
    "* *forward pass*,\n",
    "* *backward pass*,\n",
    "* powyższe w wersji batchowej (za chwilę się przekonamy, co to znaczy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa Dense\n",
    "\n",
    "<img src=\"figures/L10/dense.png\" width=400>\n",
    "\n",
    "Zakładamy, że warstwa Dense ma rozmiary oznaczone input_size oraz output_size (w powyższym przykładzie odpowiednio 5 i 3). Wewnątrz warstwy zapamiętujemy macierz wag $W$ o rozmiarze $(\\mathrm{input\\_size} + 1, \\mathrm{output\\_size})$. Pierwszy wiersz macierzy $W$ przechowuje wagi biasu (krawędzie łączące się z węzłami oznaczanymi jedynką), dlatego dodajemy go automatycznie.\n",
    "\n",
    "#### *Forward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* output poprzedniej warstwy $o^j$ - poziomy wektor o długości input_size,\n",
    "* (w wersji batchowej) batch outputów poprzedniej warstwy $X^j$ - macierz o rozmiarach (batch_size, input_size).\n",
    "\n",
    "Najpierw doklejamy jedynkę z lewej strony outputu poprzedniej warstwy:\n",
    "\n",
    "$$o^j := [1,o^j]$$\n",
    "$$X^j := \\mathrm{hstack}(\\mathrm{ones}(X^j.\\mathrm{shape}[0],1), X^j)$$\n",
    "\n",
    "zapamiętujemy zmodyfikowane wersje $o^j$ (w wersji batchowej $X^j$), będą one potrzebne do przeprowadzenia *backward pass*.\n",
    "\n",
    "Następnie obliczamy faktyczny output:\n",
    "\n",
    "$$o^{j+1} = o^jW$$\n",
    "$$X^{j+1} = X^jW$$\n",
    "\n",
    "#### *Backward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* $\\frac{\\partial L}{\\partial o^{j+1}}$ - poziomy wektor pochodnych cząstkowych długości output_size,\n",
    "* (w wersji batchowej) $\\frac{\\partial L}{\\partial X^{j+1}}$ - batch pochodnych cząstkowych o rozmiarach (batch_size, output_size).\n",
    "\n",
    "Obliczamy pochodne cząstkowe:\n",
    "$$\\frac{\\partial L}{\\partial W} = (o^j)^T\\frac{\\partial L}{\\partial o^{j+1}}$$\n",
    "$$\\frac{\\partial L}{\\partial o^j} = \\frac{\\partial L}{\\partial o^{j+1}}W$$\n",
    "\n",
    "w wersji batchowej:\n",
    "$$\\frac{\\partial L}{\\partial W} = (X^j)^T\\frac{\\partial L}{\\partial X^{j+1}}$$\n",
    "$$\\frac{\\partial L}{\\partial X^j} = \\frac{\\partial L}{\\partial X^{j+1}}W$$\n",
    "\n",
    "Do następnej warstwy propagujemy tylko $\\frac{\\partial L}{\\partial o^j}[1:]$ (w wersji batchowej $\\frac{\\partial L}{\\partial X^j}[:,1:]$) - dlaczego?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa ReLU\n",
    "\n",
    "<img src=\"figures/L10/relu.png\" width=400>\n",
    "\n",
    "Zakładamy, że warstwa ReLU ma rozmiar oznaczony layer_size == input_size == output_size (w powyższym przykładzie 4).  Warstwa aktywacji nie ma parametrów.\n",
    "\n",
    "#### *Forward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* output poprzedniej warstwy $o^j$ - poziomy wektor o długości layer_size,\n",
    "* (w wersji batchowej) batch outputów poprzedniej warstwy $X^j$ - macierz o rozmiarach (batch_size, layer_size).\n",
    "\n",
    "Następnie obliczamy output (element-wise):\n",
    "\n",
    "$$o^{j+1} = max(0, o^j)$$\n",
    "$$X^{j+1} = max(0, X^j)$$\n",
    "\n",
    "\n",
    "#### *Backward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* $\\frac{\\partial L}{\\partial o^{j+1}}$ - poziomy wektor pochodnych cząstkowych długości layer_size,\n",
    "* (w wersji batchowej) $\\frac{\\partial L}{\\partial X^{j+1}}$ - batch pochodnych cząstkowych o rozmiarach (batch_size, layer_size).\n",
    "\n",
    "Obliczamy pochodne cząstkowe:\n",
    "$$\\frac{\\partial L}{\\partial o^j} = \\frac{\\partial L}{\\partial o^{j+1}} \\circ \\mathbb{1}_{o^j>0}$$\n",
    "\n",
    "w wersji batchowej:\n",
    "$$\\frac{\\partial L}{\\partial X^j} = \\frac{\\partial L}{\\partial X^{j+1}} \\circ \\mathbb{1}_{X^j>0}$$\n",
    "\n",
    "gdzie:\n",
    "* $\\mathbb{1}_{M>0}$ to macierz, która ma jedynki tam, gdzie elementy $M$ są większe od zera, a zera w pozostałych przypadkach,\n",
    "* $\\circ$ to mnożenie macierzy element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warstwa Sigmoid\n",
    "\n",
    "<img src=\"figures/L10/sigmoid.png\" width=400>\n",
    "\n",
    "Zakładamy, że warstwa Sigmoid ma rozmiar oznaczony layer_size == input_size == output_size (w powyższym przykładzie 4).  Warstwa aktywacji nie ma parametrów.\n",
    "\n",
    "#### *Forward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* output poprzedniej warstwy $o^j$ - poziomy wektor o długości layer_size,\n",
    "* (w wersji batchowej) batch outputów poprzedniej warstwy $X^j$ - macierz o rozmiarach (batch_size, layer_size).\n",
    "\n",
    "Następnie obliczamy output (element-wise):\n",
    "\n",
    "$$o^{j+1} = \\sigma(o^j)$$\n",
    "$$X^{j+1} = \\sigma(X^j)$$\n",
    "\n",
    "gdzie:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "\n",
    "#### *Backward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* $\\frac{\\partial L}{\\partial o^{j+1}}$ - poziomy wektor pochodnych cząstkowych długości layer_size,\n",
    "* (w wersji batchowej) $\\frac{\\partial L}{\\partial X^{j+1}}$ - batch pochodnych cząstkowych o rozmiarach (batch_size, layer_size).\n",
    "\n",
    "Obliczamy pochodne cząstkowe:\n",
    "$$\\frac{\\partial L}{\\partial o^j} = \\frac{\\partial L}{\\partial o^{j+1}} \\circ \\sigma(o^j) \\circ (1-\\sigma(o^j))$$\n",
    "\n",
    "w wersji batchowej:\n",
    "$$\\frac{\\partial L}{\\partial X^j} = \\frac{\\partial L}{\\partial X^{j+1}} \\circ \\sigma(X^j) \\circ (1-\\sigma(X^j))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "\n",
    "$$L(\\mathrm{y_{true}}, \\mathrm{y_{pred}}) = L(t, \\hat y) = \\frac12(t - \\hat y)^2$$\n",
    "$$L(\\mathbf{y_{true}}, \\mathbf{y_{pred}}) = L(\\mathbf{t}, \\mathbf{\\hat y}) = \\frac{1}{\\mathrm{len}(\\mathbf{t})} \\frac12\\|t - \\hat y\\|_2^2$$\n",
    "\n",
    "#### *Forward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* $\\hat y$ - predykcję sieci neuronowej (wektor o długości output_size ostatniej warstwy, w naszych przykładach będzie to pojedyncza liczba),\n",
    "* $t$ - prawdziwą etykietę (taki sam rozmiar, jak $\\hat y$).\n",
    "\n",
    "w wersji batchowej:\n",
    "* batch predykcji sieci neuronowej $\\mathbf{\\hat y}$ - macierz o rozmiarach (batch_size, output_size),\n",
    "* batch prawdziwych etykiet $\\mathbf{t}$ - macierz o takich samych rozmiarach, jak $\\mathbf{\\hat y}$.\n",
    "\n",
    "Następnie obliczamy output (koszt) zgodnie z powyższymi wzorami.\n",
    "\n",
    "#### *Backward pass*\n",
    "\n",
    "Jest to pierwszy *backward pass*, na wejściu nie dostajemy nic.\n",
    "\n",
    "Obliczamy pochodne cząstkowe:\n",
    "$$\\frac{\\partial L}{\\partial \\hat y} = \\hat y - t$$\n",
    "\n",
    "w wersji batchowej:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{\\hat y}} = \\frac{1}{\\mathrm{len}(\\mathbf{t})} (\\mathbf{\\hat y} - \\mathbf{t})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (mean) Cross entropy\n",
    "\n",
    "$$ L(\\mathrm{y_{true}}, \\mathrm{y_{pred}}) = L(t, \\hat y) = - t \\log \\hat y - (1 - t) \\log (1 - \\hat y ) $$\n",
    "$$ L(\\mathbf{y_{true}}, \\mathbf{y_{pred}}) = L(\\mathbf{t}, \\mathbf{\\hat y}) =\\frac{1}{\\mathrm{len}(\\mathbf{t})} \\sum_{j=1}^{\\mathrm{len}(\\mathbf{t})} - \\mathbf{t}_j \\log \\mathbf{\\hat y}_j - (1 - \\mathbf{t}_j) \\log (1 - \\mathbf{\\hat y}_j ) $$\n",
    "\n",
    "#### *Forward pass*\n",
    "\n",
    "Na wejściu dostajemy:\n",
    "* $\\hat y$ - predykcję sieci neuronowej (wektor o długości output_size ostatniej warstwy, w naszych przykładach będzie to pojedyncza liczba),\n",
    "* $t$ - prawdziwą etykietę (taki sam rozmiar, jak $\\hat y$).\n",
    "\n",
    "w wersji batchowej:\n",
    "* batch predykcji sieci neuronowej $\\mathbf{\\hat y}$ - macierz o rozmiarach (batch_size, output_size),\n",
    "* batch prawdziwych etykiet $\\mathbf{t}$ - macierz o takich samych rozmiarach, jak $\\mathbf{\\hat y}$.\n",
    "\n",
    "Następnie obliczamy output (koszt) zgodnie z powyższymi wzorami.\n",
    "\n",
    "#### *Backward pass*\n",
    "\n",
    "Jest to pierwszy *backward pass*, na wejściu nie dostajemy nic.\n",
    "\n",
    "Obliczamy pochodne cząstkowe:\n",
    "$$\\frac{\\partial L}{\\partial \\hat y} = -\\frac{t}{\\hat y} + \\frac{1-t}{1-\\hat y} $$\n",
    "\n",
    "w wersji batchowej (wszystkie działania wykonujemy element-wise):\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{\\hat y}} = \\frac{1}{\\mathrm{len}(\\mathbf{t})}(-\\frac{\\mathbf{t}}{\\mathbf{\\hat y}} + \\frac{1-\\mathbf{t}}{1-\\mathbf{\\hat y}})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pochodne, których nie liczymy, a moglibyśmy\n",
    "\n",
    "Nic nie stoi na przeszkodzie, żebyśmy policzyli:\n",
    "* $\\frac{\\partial L}{\\partial y_{true}}$, bezpośrednio ze wzoru na $L$, podobnie jak w sekcji \"Wpływ neuronów z najwyższej warstwy\",\n",
    "* $\\frac{\\partial L}{\\partial o^0} = \\frac{\\partial L}{\\partial \\mathbf{x}}$, jako ostatni krok w *backward pass*.\n",
    "\n",
    "Pochodne te reprezentują siłę wpływu prawdziwej etykiety $y$ oraz cech $\\mathbf{x}$ na wartość funkcji kosztu.\n",
    "\n",
    "Pytanie kontrolne - dlaczego nie liczymy tych pochodnych?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
